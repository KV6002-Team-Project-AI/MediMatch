{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5174686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: torch in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (4.65.0)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.9 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.2.0)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.28.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from accelerate) (2.2.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from accelerate) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from accelerate) (0.3.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2023.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Pip installs\n",
    "!pip install transformers torch\n",
    "!pip install transformers[torch]\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8414573",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\"O\": 0, \"B-ALLERGY\": 1, \"I-ALLERGY\": 2}\n",
    "id2label = {0: \"O\", 1: \"B-ALLERGY\", 2: \"I-ALLERGY\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "216ba11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abdullah\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\Abdullah\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Setting path\n",
    "base_path = \"D:/Abdullah/Documents/MediMatch\"\n",
    "model_relative_path = os.path.join(\"Abdul/NLP_Models/Allergies\")\n",
    "model_path = os.path.join(base_path, model_relative_path)\n",
    "\n",
    "# Load model and tokenizers\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e45e7639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLERGY: cats\n",
      "ALLERGY: dogs\n",
      "ALLERGY: horses\n"
     ]
    }
   ],
   "source": [
    "# Example text here\n",
    "text = \"Im alergic to cats\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "predictions = logits.argmax(-1).squeeze().tolist()\n",
    "\n",
    "predicted_labels = [id2label[label] for label in predictions]\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "labeled_tokens = list(zip(tokens, predicted_labels))\n",
    "\n",
    "def aggregate_entities(tokens, predicted_labels):\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    current_label = None\n",
    "\n",
    "    for token, label in zip(tokens, predicted_labels):\n",
    "        # Skip over the special tokens\n",
    "        if token in [\"[CLS]\", \"[SEP]\"]:\n",
    "            continue\n",
    "\n",
    "        # Subword tokens\n",
    "        if token.startswith(\"##\"):\n",
    "            if current_entity:\n",
    "                current_entity[-1] += token[2:]  # Merge with the previous\n",
    "            continue\n",
    "\n",
    "        # BIO tags\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current_entity:  # Save the previous\n",
    "                entities.append((current_label, \" \".join(current_entity)))\n",
    "            current_entity = [token]\n",
    "            current_label = label[2:]  # Remove prefix\n",
    "        elif label.startswith(\"I-\") and current_label == label[2:]:\n",
    "            current_entity.append(token)\n",
    "        else:  \n",
    "            if current_entity:  # Save the previous\n",
    "                entities.append((current_label, \" \".join(current_entity)))\n",
    "                current_entity = []\n",
    "                current_label = None\n",
    "\n",
    "    if current_entity:\n",
    "        entities.append((current_label, \" \".join(current_entity)))\n",
    "\n",
    "    return entities\n",
    "\n",
    "entities = aggregate_entities(tokens, predicted_labels)\n",
    "\n",
    "def filter_entities(entities, filter_words):\n",
    "    filtered_entities = []\n",
    "    for label, entity in entities:\n",
    "        filtered_entity_words = [word for word in entity.split() if word.lower() not in filter_words]\n",
    "        if filtered_entity_words: \n",
    "            filtered_entities.append((label, \" \".join(filtered_entity_words)))\n",
    "    return filtered_entities\n",
    "\n",
    "filter_words = {\"of\", \"from\", \"to\", \"and\"}\n",
    "filtered_entities = filter_entities(entities, filter_words)\n",
    "\n",
    "# Print Outptu\n",
    "for label, entity in filtered_entities:\n",
    "    print(f\"{label}: {entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d1d86416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALLERGY: peanuts\n",
      "ALLERGY: cats\n",
      "ALLERGY: dust mites\n",
      "ALLERGY: cats , dogs\n",
      "ALLERGY: shellfish\n",
      "ALLERGY: pollen\n",
      "ALLERGY: grass\n",
      "ALLERGY: gluten\n",
      "ALLERGY: milk\n",
      "ALLERGY: latex\n",
      "ALLERGY: bananas\n",
      "ALLERGY: dogs\n",
      "ALLERGY: ragweed pollen\n",
      "ALLERGY: bee stings\n",
      "ALLERGY: shellfish\n",
      "ALLERGY: peanuts\n",
      "ALLERGY: by dust mites\n",
      "ALLERGY: mold\n",
      "ALLERGY: pollen\n",
      "ALLERGY: dairy products\n",
      "ALLERGY: eggs\n",
      "ALLERGY: shellfish\n",
      "ALLERGY: fish\n",
      "ALLERGY: peanuts\n",
      "ALLERGY: milk\n",
      "ALLERGY: pollen\n",
      "ALLERGY: wheat\n",
      "ALLERGY: beans\n",
      "ALLERGY: cheese\n",
      "ALLERGY: milk\n",
      "ALLERGY: beans\n",
      "ALLERGY: meat\n",
      "ALLERGY: chicken\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = logits.argmax(-1).squeeze().tolist()\n",
    "    predicted_labels = [id2label[label] for label in predictions]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "    labeled_tokens = list(zip(tokens, predicted_labels))\n",
    "\n",
    "    def aggregate_entities(tokens, predicted_labels):\n",
    "        entities = []\n",
    "        current_entity = []\n",
    "        current_label = None\n",
    "        for token, label in zip(tokens, predicted_labels):\n",
    "            if token in [\"[CLS]\", \"[SEP]\"]:\n",
    "                continue\n",
    "            if token.startswith(\"##\"):\n",
    "                if current_entity:\n",
    "                    current_entity[-1] += token[2:]\n",
    "                continue\n",
    "            if label.startswith(\"B-\"):\n",
    "                if current_entity:\n",
    "                    entities.append((current_label, \" \".join(current_entity)))\n",
    "                current_entity = [token]\n",
    "                current_label = label[2:]\n",
    "            elif label.startswith(\"I-\") and current_label == label[2:]:\n",
    "                current_entity.append(token)\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append((current_label, \" \".join(current_entity)))\n",
    "                    current_entity = []\n",
    "                    current_label = None\n",
    "        if current_entity:\n",
    "            entities.append((current_label, \" \".join(current_entity)))\n",
    "        return entities\n",
    "\n",
    "    entities = aggregate_entities(tokens, predicted_labels)\n",
    "\n",
    "    def filter_entities(entities, filter_words):\n",
    "        filtered_entities = []\n",
    "        for label, entity in entities:\n",
    "            filtered_entity_words = [word for word in entity.split() if word.lower() not in filter_words]\n",
    "            if filtered_entity_words:\n",
    "                filtered_entities.append((label, \" \".join(filtered_entity_words)))\n",
    "        return filtered_entities\n",
    "\n",
    "    filter_words = {\"of\", \"from\", \"to\", \"and\"}\n",
    "    filtered_entities = filter_entities(entities, filter_words)\n",
    "    return filtered_entities\n",
    "\n",
    "# Test data geneerated from CHATGPT\n",
    "test_data = [\n",
    "    \"I'm allergic to peanuts.\"\n",
    "    \"I'm allergic to cats and dust mites.\"\n",
    "    \"I'm allergic to cats, dogs, and cheese.\"\n",
    "    \"I'm allergic to shellfish.\"\n",
    "    \"I'm allergic to pollen and grass.\"\n",
    "    \"I'm allergic to gluten'.\"\n",
    "    \"I'm allergic to milk.\"\n",
    "    \"I'm allergic to latex and bananas.\"\n",
    "    \"I have a seafood allergy.\"\n",
    "    \"I'm allergic to dogs.\"\n",
    "    \"I'm allergic to ragweed pollen.\"\n",
    "    \"I have lactose intolerance.\"\n",
    "    \"I have allergies to dogs\"\n",
    "    \"I'm allergic to bee stings.\"\n",
    "    \"I'm allergic to shellfish and peanuts.\"\n",
    "    \"I have asthma triggered by dust mites.\"\n",
    "    \"I'm allergic to mold and pollen.\"\n",
    "    \"I'm allergic to dairy products.\"\n",
    "    \"I have a sesame allergy.\"\n",
    "    \"I'm allergic to eggs and soy.\"\n",
    "    \"I'm allergic to shellfish and fish.\"\n",
    "    \"I'm allergic to peanuts and milk.\"\n",
    "    \"I'm allergic to pollen, grass, and mold.\"\n",
    "    \"I'm allergic to wheat.\"\n",
    "    \"I'm allergic to beans.\"\n",
    "    \"I'm allergic to cheese and milk.\"\n",
    "    \"I'm allergic to beans.\"\n",
    "    \"I'm allergic to meat and chicken.\"\n",
    "\n",
    "    \n",
    "]\n",
    "\n",
    "# Processing each phrase\n",
    "for text in test_data:\n",
    "    filtered_entities = process_text(text)\n",
    "    for label, entity in filtered_entities:\n",
    "        print(f\"{label}: {entity}\")\n",
    "    print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c3c162f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n"
     ]
    }
   ],
   "source": [
    "def process_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = logits.argmax(-1).squeeze().tolist()\n",
    "    predicted_labels = [id2label[label] for label in predictions]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "    labeled_tokens = list(zip(tokens, predicted_labels))\n",
    "\n",
    "    def aggregate_entities(tokens, predicted_labels):\n",
    "        entities = []\n",
    "        current_entity = []\n",
    "        current_label = None\n",
    "        for token, label in zip(tokens, predicted_labels):\n",
    "            if token in [\"[CLS]\", \"[SEP]\"]:\n",
    "                continue\n",
    "            if token.startswith(\"##\"):\n",
    "                if current_entity:\n",
    "                    current_entity[-1] += token[2:]\n",
    "                continue\n",
    "            if label.startswith(\"B-\"):\n",
    "                if current_entity:\n",
    "                    entities.append((current_label, \" \".join(current_entity)))\n",
    "                current_entity = [token]\n",
    "                current_label = label[2:]\n",
    "            elif label.startswith(\"I-\") and current_label == label[2:]:\n",
    "                current_entity.append(token)\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append((current_label, \" \".join(current_entity)))\n",
    "                    current_entity = []\n",
    "                    current_label = None\n",
    "        if current_entity:\n",
    "            entities.append((current_label, \" \".join(current_entity)))\n",
    "        return entities\n",
    "\n",
    "    entities = aggregate_entities(tokens, predicted_labels)\n",
    "\n",
    "    def filter_entities(entities, filter_words):\n",
    "        filtered_entities = []\n",
    "        for label, entity in entities:\n",
    "            filtered_entity_words = [word for word in entity.split() if word.lower() not in filter_words]\n",
    "            if filtered_entity_words:\n",
    "                filtered_entities.append((label, \" \".join(filtered_entity_words)))\n",
    "        return filtered_entities\n",
    "\n",
    "    filter_words = {\"of\", \"from\", \"to\", \"and\"}\n",
    "    filtered_entities = filter_entities(entities, filter_words)\n",
    "    return filtered_entities\n",
    "\n",
    "# Test data geneerated from CHATGPT\n",
    "test_data = [\n",
    "\"I find joy in cooking healthy meals with fresh ingredients.\"\n",
    "\"I prioritize regular physical activity to maintain my fitness levels.\"\n",
    "\"I make time for hobbies that bring me joy and reduce stress.\"\n",
    "\"I prioritize self-care activities like baths and massages for relaxation.\"\n",
    "\"I enjoy family picnics as a way to bond and enjoy nutritious meals together.\"\n",
    "\"I prioritize sleep hygiene practices for better sleep and overall health.\"\n",
    "\"I make time for laughter and humor to improve my mood and well-being.\"\n",
    "\"I prioritize healthy snacks like fruits and nuts for sustained energy.\"\n",
    "\"I find joy in spending time with pets for my mental and emotional health.\"\n",
    "\"I enjoy swimming as a refreshing and low-impact form of exercise.\"\n",
    "\"I prioritize relaxation techniques like meditation for stress relief.\"\n",
    "\"I make time for regular family walks to catch up and connect.\"\n",
    "\"I prioritize regular check-ups to monitor my health and well-being.\"\n",
    "\"I find joy in exploring nature through activities like birdwatching.\"\n",
    "\"I prioritize spending time in nature to recharge and rejuvenate.\"\n",
    "\"I enjoy dancing to my favorite music as a form of exercise and self-expression.\"\n",
    "\"I prioritize spending time with loved ones to maintain strong relationships.\"\n",
    "\"I make time for hobbies like painting or crafting to relax and unwind.\"\n",
    "\"I enjoy cooking healthy meals as a way to nourish my body and soul.\"\n",
    "\"I prioritize time for relaxation and self-care to reduce stress levels.\"\n",
    "]\n",
    "\n",
    "# Processing each phrase\n",
    "for text in test_data:\n",
    "    filtered_entities = process_text(text)\n",
    "    for label, entity in filtered_entities:\n",
    "        print(f\"{label}: {entity}\")\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "761c8be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90\n",
      "Precision: 0.83\n",
      "Recall: 1.00\n",
      "Specificity: 0.80\n",
      "F1 Score: 0.91\n"
     ]
    }
   ],
   "source": [
    "# Values\n",
    "Total = 50\n",
    "True_Positives = 25\n",
    "False_Positives = 5\n",
    "total_empty = 20\n",
    "True_Negatives = 20\n",
    "False_Negatives = 0\n",
    "\n",
    "# Calculate \n",
    "accuracy = (True_Positives + True_Negatives) / Total\n",
    "precision = True_Positives / (True_Positives + False_Positives)\n",
    "recall = True_Positives / (True_Positives + False_Negatives)\n",
    "specificity = True_Negatives / (True_Negatives + False_Positives)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Specificity: {specificity:.2f}\")\n",
    "print(f\"F1 Score: {f1_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c0330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACROSS ALL 3 MODELS:\n",
    "# Average Accuracy: 0.91\n",
    "# Average Precision: 0.8033\n",
    "# Average Recall: 0.9733\n",
    "# Average Specificity: 0.7733\n",
    "# Average F1 Score: 0.88"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
