{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: torch in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (4.65.0)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.9 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.2.0)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.28.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from accelerate) (2.2.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from accelerate) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from accelerate) (0.3.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2023.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\abdullah\\anaconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Pip installs\n",
    "!pip install transformers torch\n",
    "!pip install transformers[torch]\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\"O\": 0, \"B-LIFESTYLE\": 1, \"I-LIFESTYLE\": 2}\n",
    "id2label = {0: \"O\", 1: \"B-LIFESTYLE\", 2: \"I-LIFESTYLE\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abdullah\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\Abdullah\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Setting path\n",
    "base_path = \"D:/Abdullah/Documents/MediMatch\"\n",
    "model_relative_path = os.path.join(\"Abdul/NLP_Models/Lifestyle\")\n",
    "model_path = os.path.join(base_path, model_relative_path)\n",
    "\n",
    "# Load model and tokenizers\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIFESTYLE: smoke\n",
      "LIFESTYLE: vegan\n",
      "LIFESTYLE: drink\n"
     ]
    }
   ],
   "source": [
    "# Example text here\n",
    "text = \"I smoke cigarettes, I eat vegan food, I drink alcohol\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "predictions = logits.argmax(-1).squeeze().tolist()\n",
    "\n",
    "predicted_labels = [id2label[label] for label in predictions]\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "labeled_tokens = list(zip(tokens, predicted_labels))\n",
    "\n",
    "def aggregate_entities(tokens, predicted_labels):\n",
    "    entities = []\n",
    "    current_entity = []\n",
    "    current_label = None\n",
    "\n",
    "    for token, label in zip(tokens, predicted_labels):\n",
    "        # Skip over the special tokens\n",
    "        if token in [\"[CLS]\", \"[SEP]\"]:\n",
    "            continue\n",
    "\n",
    "        # Subword tokens\n",
    "        if token.startswith(\"##\"):\n",
    "            if current_entity:\n",
    "                current_entity[-1] += token[2:]  # Merge with the previous\n",
    "            continue\n",
    "\n",
    "        # BIO tags\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current_entity:  # Save the previous\n",
    "                entities.append((current_label, \" \".join(current_entity)))\n",
    "            current_entity = [token]\n",
    "            current_label = label[2:]  # Remove prefix\n",
    "        elif label.startswith(\"I-\") and current_label == label[2:]:\n",
    "            current_entity.append(token)\n",
    "        else:  \n",
    "            if current_entity:  # Save the previous\n",
    "                entities.append((current_label, \" \".join(current_entity)))\n",
    "                current_entity = []\n",
    "                current_label = None\n",
    "\n",
    "    if current_entity:\n",
    "        entities.append((current_label, \" \".join(current_entity)))\n",
    "\n",
    "    return entities\n",
    "\n",
    "entities = aggregate_entities(tokens, predicted_labels)\n",
    "\n",
    "def filter_entities(entities, filter_words):\n",
    "    filtered_entities = []\n",
    "    for label, entity in entities:\n",
    "        filtered_entity_words = [word for word in entity.split() if word.lower() not in filter_words]\n",
    "        if filtered_entity_words: \n",
    "            filtered_entities.append((label, \" \".join(filtered_entity_words)))\n",
    "    return filtered_entities\n",
    "\n",
    "filter_words = {\"of\", \"from\", \"to\", \"and\", \"eat\"}\n",
    "filtered_entities = filter_entities(entities, filter_words)\n",
    "\n",
    "# Print Outptu\n",
    "for label, entity in filtered_entities:\n",
    "    print(f\"{label}: {entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIFESTYLE: exercise\n",
      "LIFESTYLE: vegan\n",
      "LIFESTYLE: smoke\n",
      "LIFESTYLE: vegetarian\n",
      "LIFESTYLE: drink\n",
      "LIFESTYLE: exercise\n",
      "LIFESTYLE: exercise\n",
      "LIFESTYLE: gluten - free\n",
      "LIFESTYLE: smoke\n",
      "LIFESTYLE: drink\n",
      "LIFESTYLE: exercise\n",
      "LIFESTYLE: smoke\n",
      "LIFESTYLE: exercise\n",
      "LIFESTYLE: smoke\n",
      "LIFESTYLE: drink\n",
      "LIFESTYLE: smoke\n",
      "LIFESTYLE: drink alcohol\n",
      "LIFESTYLE: exercise\n",
      "LIFESTYLE: vegan\n",
      "LIFESTYLE: exercise\n",
      "LIFESTYLE: exercise\n",
      "LIFESTYLE: gluten - free\n",
      "LIFESTYLE: exercise regularly\n",
      "LIFESTYLE: smoke\n",
      "LIFESTYLE: drink alcohol\n",
      "LIFESTYLE: exercise\n",
      "LIFESTYLE: halal\n",
      "LIFESTYLE: smoke\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = logits.argmax(-1).squeeze().tolist()\n",
    "    predicted_labels = [id2label[label] for label in predictions]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "    labeled_tokens = list(zip(tokens, predicted_labels))\n",
    "\n",
    "    def aggregate_entities(tokens, predicted_labels):\n",
    "        entities = []\n",
    "        current_entity = []\n",
    "        current_label = None\n",
    "        for token, label in zip(tokens, predicted_labels):\n",
    "            if token in [\"[CLS]\", \"[SEP]\"]:\n",
    "                continue\n",
    "            if token.startswith(\"##\"):\n",
    "                if current_entity:\n",
    "                    current_entity[-1] += token[2:]\n",
    "                continue\n",
    "            if label.startswith(\"B-\"):\n",
    "                if current_entity:\n",
    "                    entities.append((current_label, \" \".join(current_entity)))\n",
    "                current_entity = [token]\n",
    "                current_label = label[2:]\n",
    "            elif label.startswith(\"I-\") and current_label == label[2:]:\n",
    "                current_entity.append(token)\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append((current_label, \" \".join(current_entity)))\n",
    "                    current_entity = []\n",
    "                    current_label = None\n",
    "        if current_entity:\n",
    "            entities.append((current_label, \" \".join(current_entity)))\n",
    "        return entities\n",
    "\n",
    "    entities = aggregate_entities(tokens, predicted_labels)\n",
    "\n",
    "    def filter_entities(entities, filter_words):\n",
    "        filtered_entities = []\n",
    "        for label, entity in entities:\n",
    "            filtered_entity_words = [word for word in entity.split() if word.lower() not in filter_words]\n",
    "            if filtered_entity_words:\n",
    "                filtered_entities.append((label, \" \".join(filtered_entity_words)))\n",
    "        return filtered_entities\n",
    "\n",
    "    filter_words = {\"and\", \"eat\", \"diet\", \"follow\", \"a\"}\n",
    "    filtered_entities = filter_entities(entities, filter_words)\n",
    "    return filtered_entities\n",
    "\n",
    "# Test data geneerated from CHATGPT\n",
    "test_data = [\n",
    "    \"I exercise and follow a vegan diet.\"\n",
    "    \"I smoke and follow a vegetarian diet.\"\n",
    "    \"I drink alcohol and exercise.\"\n",
    "    \"I exercise and follow a gluten-free diet.\"\n",
    "    \"I smoke and follow a vegan diet.\"\n",
    "    \"I drink alcohol and follow a vegetarian diet.\"\n",
    "    \"I exercise regularly.\"\n",
    "    \"I smoke and drink alcohol.\"\n",
    "    \"I exercise and follow a halal diet.\"\n",
    "    \"I smoke and follow a halal diet.\"\n",
    "    \"I drink alcohol.\"\n",
    "    \"I smoke.\"\n",
    "    \"I drink alcohol and follow a vegan diet.\"\n",
    "    \"I exercise and follow a vegetarian diet.\"\n",
    "    \"I'm Vegan.\"\n",
    "    \"Im Vegetarian.\"\n",
    "    \"I exercise.\"\n",
    "    \"I exercise and follow a gluten-free diet.\"\n",
    "    \"I exercise regularly.\"\n",
    "    \"I smoke and drink alcohol.\"\n",
    "    \"I exercise and follow a halal diet.\"\n",
    "    \"I smoke and follow a halal diet.\"\n",
    "]\n",
    "\n",
    "# Processing each phrase\n",
    "for text in test_data:\n",
    "    filtered_entities = process_text(text)\n",
    "    for label, entity in filtered_entities:\n",
    "        print(f\"{label}: {entity}\")\n",
    "    print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIFESTYLE: lactose intolerance\n",
      "LIFESTYLE: shellfish\n",
      "LIFESTYLE: triggered by\n",
      "LIFESTYLE: shellfish\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "def process_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions = logits.argmax(-1).squeeze().tolist()\n",
    "    predicted_labels = [id2label[label] for label in predictions]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "    labeled_tokens = list(zip(tokens, predicted_labels))\n",
    "\n",
    "    def aggregate_entities(tokens, predicted_labels):\n",
    "        entities = []\n",
    "        current_entity = []\n",
    "        current_label = None\n",
    "        for token, label in zip(tokens, predicted_labels):\n",
    "            if token in [\"[CLS]\", \"[SEP]\"]:\n",
    "                continue\n",
    "            if token.startswith(\"##\"):\n",
    "                if current_entity:\n",
    "                    current_entity[-1] += token[2:]\n",
    "                continue\n",
    "            if label.startswith(\"B-\"):\n",
    "                if current_entity:\n",
    "                    entities.append((current_label, \" \".join(current_entity)))\n",
    "                current_entity = [token]\n",
    "                current_label = label[2:]\n",
    "            elif label.startswith(\"I-\") and current_label == label[2:]:\n",
    "                current_entity.append(token)\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append((current_label, \" \".join(current_entity)))\n",
    "                    current_entity = []\n",
    "                    current_label = None\n",
    "        if current_entity:\n",
    "            entities.append((current_label, \" \".join(current_entity)))\n",
    "        return entities\n",
    "\n",
    "    entities = aggregate_entities(tokens, predicted_labels)\n",
    "\n",
    "    def filter_entities(entities, filter_words):\n",
    "        filtered_entities = []\n",
    "        for label, entity in entities:\n",
    "            filtered_entity_words = [word for word in entity.split() if word.lower() not in filter_words]\n",
    "            if filtered_entity_words:\n",
    "                filtered_entities.append((label, \" \".join(filtered_entity_words)))\n",
    "        return filtered_entities\n",
    "\n",
    "    filter_words = {\"of\", \"from\", \"to\", \"and\"}\n",
    "    filtered_entities = filter_entities(entities, filter_words)\n",
    "    return filtered_entities\n",
    "\n",
    "# Test data geneerated from CHATGPT\n",
    "test_data = [\n",
    "    \"I'm allergic to peanuts.\"\n",
    "    \"I'm allergic to cats and dust mites.\"\n",
    "    \"I'm allergic to cats, dogs, and cheese.\"\n",
    "    \"I'm allergic to shellfish.\"\n",
    "    \"I'm allergic to pollen and grass.\"\n",
    "    \"I'm allergic to gluten'.\"\n",
    "    \"I'm allergic to milk.\"\n",
    "    \"I'm allergic to latex and bananas.\"\n",
    "    \"I have a seafood allergy.\"\n",
    "    \"I'm allergic to dogs.\"\n",
    "    \"I'm allergic to ragweed pollen.\"\n",
    "    \"I have lactose intolerance.\"\n",
    "    \"I have allergies to dogs\"\n",
    "    \"I'm allergic to bee stings.\"\n",
    "    \"I'm allergic to shellfish and peanuts.\"\n",
    "    \"I have asthma triggered by dust mites.\"\n",
    "    \"I'm allergic to mold and pollen.\"\n",
    "    \"I'm allergic to dairy products.\"\n",
    "    \"I have a sesame allergy.\"\n",
    "    \"I'm allergic to eggs and soy.\"\n",
    "    \"I'm allergic to shellfish and fish.\"\n",
    "    \"I'm allergic to peanuts and milk.\"\n",
    "    \"I'm allergic to pollen, grass, and mold.\"\n",
    "    \"I'm allergic to chicken.\"\n",
    "    \"I'm allergic to pollen.\"\n",
    "]\n",
    "\n",
    "# Processing each phrase\n",
    "for text in test_data:\n",
    "    filtered_entities = process_text(text)\n",
    "    for label, entity in filtered_entities:\n",
    "        print(f\"{label}: {entity}\")\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n",
      "Precision: 0.82\n",
      "Recall: 0.92\n",
      "Specificity: 0.71\n",
      "F1 Score: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Values\n",
    "Total = 59\n",
    "True_Positives = 36\n",
    "False_Positives = 8\n",
    "total_empty = 25\n",
    "True_Negatives = 21\n",
    "False_Negatives = 4\n",
    "\n",
    "# Calculate \n",
    "accuracy = (True_Positives + True_Negatives) / Total\n",
    "precision = True_Positives / (True_Positives + False_Positives)\n",
    "recall = True_Positives / (True_Positives + False_Negatives)\n",
    "specificity = True_Negatives / (True_Negatives + False_Positives)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Specificity: {specificity:.2f}\")\n",
    "print(f\"F1 Score: {f1_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACROSS ALL 3 MODELS:\n",
    "# Average Accuracy: 0.91\n",
    "# Average Precision: 0.8033\n",
    "# Average Recall: 0.9733\n",
    "# Average Specificity: 0.7733\n",
    "# Average F1 Score: 0.88"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
